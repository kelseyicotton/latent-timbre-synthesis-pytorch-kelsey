{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# -*- coding: utf-8 -*-\r\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\r\n",
    "\r\n",
    "from torch import nn, optim\r\n",
    "from torch.nn import functional as F\r\n",
    "from torchvision import datasets, transforms\r\n",
    "\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import os, sys, argparse, time\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "import librosa\r\n",
    "import soundfile as sf\r\n",
    "import configparser\r\n",
    "import random\r\n",
    "import json\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pdb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class VAE(nn.Module):\r\n",
    "  def __init__(self, n_bins, n_units, latent_dim):\r\n",
    "    super(VAE, self).__init__()\r\n",
    "\r\n",
    "    self.n_bins = n_bins\r\n",
    "    self.n_units = n_units\r\n",
    "    self.latent_dim = latent_dim\r\n",
    "    \r\n",
    "    self.fc1 = nn.Linear(n_bins, n_units)\r\n",
    "    self.fc21 = nn.Linear(n_units, latent_dim)\r\n",
    "    self.fc22 = nn.Linear(n_units, latent_dim)\r\n",
    "    self.fc3 = nn.Linear(latent_dim, n_units)\r\n",
    "    self.fc4 = nn.Linear(n_units, n_bins)\r\n",
    "\r\n",
    "  def encode(self, x):\r\n",
    "      h1 = F.relu(self.fc1(x))\r\n",
    "      return self.fc21(h1), self.fc22(h1)\r\n",
    "\r\n",
    "  def reparameterize(self, mu, logvar):\r\n",
    "      std = torch.exp(0.5*logvar)\r\n",
    "      eps = torch.randn_like(std)\r\n",
    "      return mu + eps*std\r\n",
    "\r\n",
    "  def decode(self, z):\r\n",
    "      h3 = F.relu(self.fc3(z))\r\n",
    "      return F.relu(self.fc4(h3))\r\n",
    "\r\n",
    "  def forward(self, x):\r\n",
    "      mu, logvar = self.encode(x.view(-1, self.n_bins))\r\n",
    "      z = self.reparameterize(mu, logvar)\r\n",
    "      return self.decode(z), mu, logvar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sampling_rate = 44100\r\n",
    "n_bins = 384\r\n",
    "n_units = 2048\r\n",
    "latent_dim = 256\r\n",
    "device = 'cuda:0'\r\n",
    "\r\n",
    "batch_size = 256\r\n",
    "\r\n",
    "dataset = Path(r'D:\\datasets\\Audio\\latent-timbre-synthesis\\erokia')\r\n",
    "my_test_audio = dataset / 'test_audio'\r\n",
    "my_cqt = dataset / 'npy'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# LOAD FROM CHECKPOINT - PASS IF YOU WOULD LIKE TO LOAD FROM MODEL\r\n",
    "\r\n",
    "state = torch.load(Path(r\"D:\\datasets\\Audio\\latent-timbre-synthesis\\erokia\\lts-pytorch\\run-001\\model\\checkpoints\\ckpt_00400\"))\r\n",
    "model = VAE(n_bins, n_units, latent_dim).to(device)\r\n",
    "model.load_state_dict(state['state_dict'])\r\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "  (fc21): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc22): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# LOAD FROM MODEL\r\n",
    "model = VAE(n_bins, n_units, latent_dim).to(device)\r\n",
    "model = torch.load(Path(r'D:\\datasets\\Audio\\latent-timbre-synthesis\\erokia\\lts-pytorch\\run-001\\model\\last_model.pt'))\r\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "  (fc21): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc22): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# List the test audio files from the dataset\r\n",
    "test_files = [f for f in my_test_audio.glob('*.wav')]\r\n",
    "init = True\r\n",
    "\r\n",
    "for test in test_files:\r\n",
    "    \r\n",
    "    audio_full, _ = librosa.load(test, sr=sampling_rate)\r\n",
    "    dataname = Path(test).stem\r\n",
    "    cqt_full = np.load(my_cqt.joinpath(dataname + '.npy'))\r\n",
    "\r\n",
    "    if init:\r\n",
    "        test_dataset_audio = audio_full\r\n",
    "        test_dataset_cqt = cqt_full\r\n",
    "        init = False\r\n",
    "    else:\r\n",
    "        test_dataset_audio = np.concatenate((test_dataset_audio, audio_full ),axis=0)\r\n",
    "        test_dataset_cqt = np.concatenate((test_dataset_cqt, cqt_full ),axis=0)\r\n",
    "\r\n",
    "# Create a dataloader for test dataset\r\n",
    "test_tensor = torch.Tensor(test_dataset_cqt)\r\n",
    "test_dataset = TensorDataset(test_tensor)\r\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "test_dataset_cqt.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15411, 384)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "init_test = True\r\n",
    "for iterno, test_tuple in enumerate(test_dataloader):\r\n",
    "    test_sample, = test_tuple\r\n",
    "    with torch.no_grad():\r\n",
    "        test_sample = test_sample.cuda()\r\n",
    "        test_pred_z = model.encode(test_sample)\r\n",
    "        test_pred = model.decode(test_pred_z[0])\r\n",
    "    if init_test:\r\n",
    "        test_predictions = test_pred\r\n",
    "        init_test = False\r\n",
    "    else:\r\n",
    "        test_predictions = torch.cat((test_predictions, test_pred ),0)\r\n",
    "\r\n",
    "y_inv_32 = librosa.griffinlim_cqt(test_predictions.permute(1,0).cpu().numpy(), sr=sampling_rate, n_iter=4, hop_length=128, bins_per_octave=48, dtype=np.float32)\r\n",
    "sf.write('test_reconst.wav', y_inv_32, sampling_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "sf.write('test_reconst.wav', y_inv_32, sampling_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model.encode(test_sample)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "s = np.random.rand(np.random.randint(1024))\r\n",
    "if s.shape[0] % 1024 != 0:\r\n",
    "    num_zeros = 1024 - s.shape[0] % 1024\r\n",
    "    s = np.pad(s, (0, num_zeros), 'constant', constant_values=(0,0))\r\n",
    "\r\n",
    "s.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "cqt_fmin_hz = librosa.note_to_hz('A1')\r\n",
    "cqt_fmin_samples = 1 / cqt_fmin_hz * sampling_rate\r\n",
    "cqt_fmin_samples"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "801.8181818181818"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "s = np.random.rand(np.random.randint(1024))\r\n",
    "print(s.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(192,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('pt181': conda)"
  },
  "interpreter": {
   "hash": "cebee7b4cdaacce6cf6c53cf0ef46aa2eb1e162cf422a6dbfd292967c5d07d8d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}