{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, argparse, time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import configparser\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from ltsp.model import VAE, loss_function\n",
    "from ltsp.tests import init_test_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 44100\n",
    "n_bins = 384\n",
    "n_units = 2048\n",
    "latent_dim = 256\n",
    "device = 'cuda:0'\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset = Path(r'C:\\Users\\Kivanc\\Documents\\my_workspace\\datasets\\latent-timbre-synthesis-pytorch-2\\erokia')\n",
    "my_test_audio = dataset / 'test_audio'\n",
    "my_cqt = dataset / 'npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "  (fc21): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc22): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD FROM MODEL\n",
    "model = VAE(n_bins, n_units, latent_dim).to(device)\n",
    "model_path = dataset / 'lts-pytorch' / 'run-004' / 'model' / 'best_model.pt'\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "  (fc21): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc22): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DONT RUN THIS\n",
    "# LOAD FROM CHECKPOINT - PASS IF YOU WOULD LIKE TO LOAD FROM MODEL\n",
    "\n",
    "state = torch.load(Path(r'C:\\Users\\Kivanc\\Documents\\my_workspace\\datasets\\latent-timbre-synthesis-pytorch-2\\erokia\\lts-pytorch\\run-004\\model\\checkpoints\\ckpt_00500'))\n",
    "model = VAE(n_bins, n_units, latent_dim).to(device)\n",
    "model.load_state_dict(state['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the test audio files from the dataset\n",
    "test_files = [f for f in my_test_audio.glob('*.wav')]\n",
    "init = True\n",
    "\n",
    "for test in test_files:\n",
    "    \n",
    "    audio_full, _ = librosa.load(test, sr=sampling_rate)\n",
    "    dataname = Path(test).stem\n",
    "    cqt_full = np.load(my_cqt.joinpath(dataname + '.npy'))\n",
    "\n",
    "    if init:\n",
    "        test_dataset_audio = audio_full\n",
    "        test_dataset_cqt = cqt_full\n",
    "        init = False\n",
    "    else:\n",
    "        test_dataset_audio = np.concatenate((test_dataset_audio, audio_full ),axis=0)\n",
    "        test_dataset_cqt = np.concatenate((test_dataset_cqt, cqt_full ),axis=0)\n",
    "\n",
    "# Create a dataloader for test dataset\n",
    "test_tensor = torch.Tensor(test_dataset_cqt)\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "for iterno, test_tuple in enumerate(test_dataloader):\n",
    "    test_sample, = test_tuple\n",
    "    with torch.no_grad():\n",
    "        test_sample = test_sample.cuda()\n",
    "        test_pred_z = model.encode(test_sample.double())\n",
    "        test_pred = model.decode(test_pred_z[0])\n",
    "    if init_test:\n",
    "        test_predictions = test_pred\n",
    "        init_test = False\n",
    "    else:\n",
    "        test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "\n",
    "y_inv_32 = librosa.griffinlim_cqt(test_predictions.permute(1,0).cpu().numpy(), sr=sampling_rate, n_iter=1, hop_length=128, bins_per_octave=48, dtype=np.float32)\n",
    "sf.write('test_reconst.wav', y_inv_32, sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1, test_dataset2 = torch.tensor_split(test_tensor, 2)\n",
    "test_dataloader1 = DataLoader(test_dataset1, batch_size = batch_size, shuffle=False)\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "for iterno, test_sample in enumerate(test_dataloader1):\n",
    "    with torch.no_grad():\n",
    "        test_sample = test_sample.double().to(device)\n",
    "        test1_mu, test1_logvar = model.encode(test_sample)\n",
    "        test1_z = model.reparameterize(test1_mu, test1_logvar)\n",
    "        \n",
    "    if init_test:\n",
    "        test1_z_all = test1_z \n",
    "        init_test = False\n",
    "\n",
    "    else:\n",
    "        test1_z_all = torch.cat((test1_z_all, test1_z),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "for iterno, test_sample in enumerate(test_dataloader2):\n",
    "    with torch.no_grad():\n",
    "        test_sample = test_sample.double().to(device)\n",
    "        test2_mu, test2_logvar = model.encode(test_sample)\n",
    "        test2_z = model.reparameterize(test2_mu, test2_logvar)\n",
    "\n",
    "    if init_test:\n",
    "        test2_z_all = test2_z \n",
    "        init_test = False\n",
    "\n",
    "    else:\n",
    "        test2_z_all = torch.cat((test2_z_all, test2_z ),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructions(dataloader, model, save = True, path = './reconstruction.wav', sampling_rate = 44100):\n",
    "    init_test = True\n",
    "        \n",
    "    for iterno, test_sample in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_sample = test_sample.double().to(device)\n",
    "            test_pred, _, _ = model(test_sample)\n",
    "\n",
    "        if init_test:\n",
    "            test_predictions = test_pred\n",
    "            init_test = False\n",
    "\n",
    "        else:\n",
    "            test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "    \n",
    "    if save:\n",
    "        outpath = Path(path)\n",
    "        y_inv_32 = librosa.griffinlim_cqt(test_predictions.permute(1,0).cpu().numpy(), sr=sampling_rate, n_iter=1, hop_length=128, bins_per_octave=48, dtype=np.float32)\n",
    "        sf.write( outpath, y_inv_32, sampling_rate)\n",
    "    else:\n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions(test_dataloader1, model, path = './test1_original.wav' )\n",
    "reconstructions(test_dataloader2, model, path = './test2_original.wav' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (mu1 * a) + (mu2 * (1-a)) \n",
    "inter_amount = 0.5\n",
    "inter_z = torch.add( torch.mul(test1_z_all, (1-inter_amount)), torch.mul(test2_z_all, inter_amount) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "      \n",
    "with torch.no_grad():\n",
    "    test_pred = model.decode(inter_z)\n",
    "\n",
    "if init_test:\n",
    "    test_predictions = test_pred\n",
    "    init_test = False\n",
    "\n",
    "else:\n",
    "    test_predictions = torch.cat((test_predictions, test_pred ),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = Path('./inter-0.5.wav')\n",
    "y_inv_32 = librosa.griffinlim_cqt(test_predictions.permute(1,0).cpu().numpy(), sr=sampling_rate, n_iter=1, hop_length=128, bins_per_octave=48, dtype=np.float32)\n",
    "sf.write( outpath, y_inv_32, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cebee7b4cdaacce6cf6c53cf0ef46aa2eb1e162cf422a6dbfd292967c5d07d8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('pt181': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
