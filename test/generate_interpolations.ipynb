{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os, sys, argparse, time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import configparser\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from cabbage.model import VAE, loss_function\n",
    "from cabbage.tests import init_test_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 44100\n",
    "n_bins = 384\n",
    "n_units = 2048\n",
    "latent_dim = 256\n",
    "device = 'cuda:0'\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset = Path(r'C:\\Users\\Kivanc\\Documents\\my_workspace\\datasets\\latent-timbre-synthesis-pytorch-2\\erokia')\n",
    "my_test_audio = dataset / 'test_audio'\n",
    "my_cqt = dataset / 'npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "  (fc21): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc22): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD FROM MODEL\n",
    "model = VAE(n_bins, n_units, latent_dim).to(device)\n",
    "model_path = dataset / 'lts-pytorch' / 'run-004' / 'model' / 'best_model.pt'\n",
    "model = torch.load(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN THIS\n",
    "# LOAD FROM CHECKPOINT - PASS IF YOU WOULD LIKE TO LOAD FROM MODEL\n",
    "\n",
    "state = torch.load(Path(r\"D:\\datasets\\Audio\\spectralvae\\erokia\\spectralvae\\run-004\\model\\best_model.pt\"))\n",
    "model = VAE(n_bins, n_units, latent_dim).to(device)\n",
    "model.load_state_dict(state['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the test audio files from the dataset\n",
    "test_files = [f for f in my_test_audio.glob('*.wav')]\n",
    "init = True\n",
    "\n",
    "for test in test_files:\n",
    "    \n",
    "    audio_full, _ = librosa.load(test, sr=sampling_rate)\n",
    "    dataname = Path(test).stem\n",
    "    cqt_full = np.load(my_cqt.joinpath(dataname + '.npy'))\n",
    "\n",
    "    if init:\n",
    "        test_dataset_audio = audio_full\n",
    "        test_dataset_cqt = cqt_full\n",
    "        init = False\n",
    "    else:\n",
    "        test_dataset_audio = np.concatenate((test_dataset_audio, audio_full ),axis=0)\n",
    "        test_dataset_cqt = np.concatenate((test_dataset_cqt, cqt_full ),axis=0)\n",
    "\n",
    "# Create a dataloader for test dataset\n",
    "test_tensor = torch.Tensor(test_dataset_cqt)\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "for iterno, test_tuple in enumerate(test_dataloader):\n",
    "    test_sample, = test_tuple\n",
    "    with torch.no_grad():\n",
    "        test_sample = test_sample.cuda()\n",
    "        test_pred_z = model.encode(test_sample.double())\n",
    "        test_pred = model.decode(test_pred_z[0])\n",
    "    if init_test:\n",
    "        test_predictions = test_pred\n",
    "        init_test = False\n",
    "    else:\n",
    "        test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "\n",
    "y_inv_32 = librosa.griffinlim_cqt(test_predictions.permute(1,0).cpu().numpy(), sr=sampling_rate, n_iter=1, hop_length=128, bins_per_octave=48, dtype=np.float32)\n",
    "sf.write('test_reconst.wav', y_inv_32, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TestDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20776/2616529354.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dataset1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_audio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_audio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataloader1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_dataset2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_audio\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_audio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_dataloader2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TestDataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset1 = TestDataset(test_dataset_audio[:len(test_dataset_audio)//2], segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "test_dataloader1 = DataLoader(test_dataset1, batch_size = batch_size, shuffle=False)\n",
    "test_dataset2 = TestDataset(test_dataset_audio[len(test_dataset_audio)//2:], segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "for iterno, test_sample in enumerate(test_dataloader1):\n",
    "    with torch.no_grad():\n",
    "        test_sample = test_sample.to(device)\n",
    "        test1_mu, test1_logvar = model.encode(test_sample)\n",
    "\n",
    "    if init_test:\n",
    "        test1_z_mu = test1_mu \n",
    "        test1_z_logvar = test1_logvar\n",
    "        init_test = False\n",
    "\n",
    "    else:\n",
    "        test1_z_mu = torch.cat((test1_z_mu, test1_mu ),0)\n",
    "        test1_z_logvar = torch.cat((test1_z_logvar, test1_logvar ),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test = True\n",
    "for iterno, test_sample in enumerate(test_dataloader2):\n",
    "    with torch.no_grad():\n",
    "        test_sample = test_sample.to(device)\n",
    "        test2_mu, test2_logvar = model.encode(test_sample)\n",
    "\n",
    "    if init_test:\n",
    "        test2_z_mu = test2_mu \n",
    "        test2_z_logvar = test2_logvar\n",
    "        init_test = False\n",
    "\n",
    "    else:\n",
    "        test2_z_mu = torch.cat((test2_z_mu, test2_mu ),0)\n",
    "        test2_z_logvar = torch.cat((test2_z_logvar, test2_logvar ),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([926, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_z_mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions(test_dataloader1, model, path = './test1_original.wav' )\n",
    "reconstructions(test_dataloader2, model, path = './test2_original.wav' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (mu1 * a) + (mu2 * (1-a)) \n",
    "inter_amount = 0.5\n",
    "inter_z_mu = torch.add( torch.mul(test1_z_mu, (1-inter_amount)), torch.mul(test2_z_mu, inter_amount) )\n",
    "inter_z_logvar = torch.add( torch.mul(test1_z_logvar, (1-inter_amount)), torch.mul(test2_z_logvar, inter_amount) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kivan\\anaconda3\\envs\\pt181\\lib\\site-packages\\torch\\nn\\functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "init_test = True\n",
    "      \n",
    "with torch.no_grad():\n",
    "    test_pred_z = model.reparameterize(inter_z_mu, inter_z_logvar)\n",
    "    test_pred = model.decode(test_pred_z)\n",
    "\n",
    "if init_test:\n",
    "    test_predictions = test_pred\n",
    "    init_test = False\n",
    "\n",
    "else:\n",
    "    test_predictions = torch.cat((test_predictions, test_pred ),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = Path('./inter-0.5.wav')\n",
    "sf.write( outpath, test_predictions.view(-1).cpu().numpy(), sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cebee7b4cdaacce6cf6c53cf0ef46aa2eb1e162cf422a6dbfd292967c5d07d8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('pt181': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
