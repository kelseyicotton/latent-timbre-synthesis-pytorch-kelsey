
[audio]
sampling_rate = 44100
hop_length = 128

bins_per_octave = 48
num_octaves = 8
cqt_fmin = A1
cqt_bit_depth = float32

n_iter = 4

[dataset]
datapath = /mimer/NOBACKUP/groups/ml-music/datasets/lts-pytorch/erokia
cqt_dataset = npy
test_dataset = test_audio
generate_test = True
overwrite_cqt = True
check_audio = True
check_dataset = True
workspace = 
run_number = 0
total_frames = 

[VAE]
latent_dim = 256
n_units = 2048
kl_beta = 0.0005
device = cuda:0

[training]
epochs = 3000
save_best_model_after = 500
learning_rate = 0.00005
batch_size = 64
checkpoint_interval = 200

[notes]
additional_notes = debugging, replicating run-013 from old lts, fixed kl_beta, batch_size, batch_norm

[extra]
normalize_examples = True
example_length = 10
plot_model = True

description = alvis
start = 
end = 
time_elapsed = 
